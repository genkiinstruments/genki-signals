{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337c43a4",
   "metadata": {},
   "source": [
    "# Signal functions in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7ec09",
   "metadata": {},
   "source": [
    "* Geometry example, raw acc/gyro data from Wave and compute pose, 3D cube? Gravity? Linear acceleration?\n",
    "* Filtering / spectrogram example with mic data / wave forms\n",
    "* ML inference example\n",
    "* Windowed signal functions: delay? FFT?\n",
    "* Creating custom signal functions, serialization quirks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49de56",
   "metadata": {},
   "source": [
    "`SignalFunction`s are functions that take in one or more sigals and return a new signal. They can be one of to types:\n",
    "\n",
    "* one-to-one functions take in one sample and return one sample. They can be stateful and depend on previous samples, but the number of samples out should always be the same as number of samples in.\n",
    "* many-to-one (Windowed) functions operate on a sliding window of samples. The windows can overlap, but the function only returns one value per window. They can still receive a single sample at a time but will only return a value when the window has been filled. As well as defining what to do with the whow to process a window they need to define the window length window overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb63c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.system import System\n",
    "from genki_signals.sources import MicSource, CameraSource, Sampler\n",
    "import genki_signals.functions as f\n",
    "from genki_signals.frontends import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252077a8",
   "metadata": {},
   "source": [
    "A good example of a windowed signal function is short-time Fourier transform (stft) which computes the Discrete Fourier transform on a sliding window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d75864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = MicSource()\n",
    "stft = f.FourierTransform(\"audio\", \"fourier\", window_size=1024, window_overlap=512)\n",
    "fourier_system = System(mic, [stft])\n",
    "\n",
    "fourier_system.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64554b",
   "metadata": {},
   "source": [
    "One way to view data from a `System` is to connect it to a buffer with `System.register_data_feed`. This can be useful to see if a `SignalFunction` is behaving properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f273f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.buffers import DataBuffer\n",
    "\n",
    "buffer = DataBuffer()\n",
    "\n",
    "fourier_system.register_data_feed(id(buffer), lambda d: buffer.extend(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d96659",
   "metadata": {},
   "source": [
    "Now all data that goes through our `System` will be sent to the buffer. Below we can view our buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84b7df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBuffer(max_size=None, data=timestamp: (49,)\n",
       "audio: (50176,)\n",
       "fourier: (513, 98))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfeb6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b23ca",
   "metadata": {},
   "source": [
    "Note that in this case, the time series called `'fourier'` is shorter than `'audio'` - it is downsampled. The `DataBuffer` will accept data like this, but with great power comes great reponsibility. This can be a source of bugs since one tends to assume that timeseries grouped together are operating on the same frequency.\n",
    "\n",
    "Another example of a `SignalFunction` are the `Inference` and `WindowedInference` functions which take in a onnx file and calculate real-time inference with the neural network.\n",
    "\n",
    "Let's load a model which predicts the relative depth of an image. For this example to work you need to have [pytorch](https://pytorch.org/) and [timm](https://pypi.org/project/timm/) installed.\n",
    "\n",
    "(Note: You can also create your own `Inference` function but more on custom `SignalFunctions` later.)\n",
    "\n",
    "We start by loading the model and creating an ONNX file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a7eb1e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/egill/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/egill/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#load the model from torch.hub\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "\n",
    "input_resolution = (256, 256)\n",
    "dummy_input = torch.randn((1, 3, *input_resolution))\n",
    "model_path = \"./midas.onnx\"\n",
    "\n",
    "# export the model to onnx\n",
    "torch.onnx.export(\n",
    "    midas,\n",
    "    dummy_input,\n",
    "    model_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    # dynamic_axes=({\"input\": [0]})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12355",
   "metadata": {},
   "source": [
    "To connect the model to our camera we need to create a `CameraSource`, a `Sampler` to sample it, and connect it to a `System` which also computes the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f49990",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraSource(resolution=input_resolution)\n",
    "camera_sampler = Sampler({\"model_input\": camera}, 10)\n",
    "model_inference = f.Inference(\"model_input\", \"model_output\", model_path, stateful=False)\n",
    "inference_system = System(camera_sampler, [model_inference], update_rate=50)\n",
    "inference_system.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cba5e5",
   "metadata": {},
   "source": [
    "The `Inference` function takes in an input name, output name, model path, and the boolean parameter `stateful`. When `stateful` is `False`, the model works on one input sample at a time independently. \n",
    "\n",
    "With `stateful=True`, the model is assumed to also take in a state vector as input, and output a new state vector along with the output (i.e. it is a Recurrent Neural Network), allowing it to operate with some historical context.\n",
    "\n",
    "Now the depth image is under the key \"model_output\" and we can visualize it with our `WidgetFrontend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bdc3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6aeb97a4f54940a625b303b1c42802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg'), Image(value=b'', format='jpeg'))), HBox()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genki_signals.frontends import Video, WidgetFrontend\n",
    "\n",
    "video = Video(\"model_input\")\n",
    "depth = Video(\"model_output\")\n",
    "\n",
    "frontend = WidgetFrontend(inference_system, [video, depth])\n",
    "\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4171b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68aee6b",
   "metadata": {},
   "source": [
    "Notes on using multiple signal functions:\n",
    "\n",
    "If one `SignalFunction` depends on the output of another `SignalFunction` then it needs to come after the other in the list of functions when initializing the `System`\n",
    "\n",
    "An example of a sequence of `SignalFunction`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51a1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_vel = Differentiate(\"mouse_pos\", \"timestamp\", \"mouse_vel\")\n",
    "vel_to_acc = Differentiate(\"mouse_vel\", \"timestamp\", \"mouse_acc\")\n",
    "acc_to_vel = Integrate(\"mouse_acc\", \"timestamp\", \"mouse_vel_2\")\n",
    "vel_to_pos = Integrate(\"mouse_vel_2\", \"timestamp\", \"mouse_pos_2\", use_trapz=False)\n",
    "\n",
    "mouse_source = MouseSource()\n",
    "sampler = Sampler({\"mouse_pos\": mouse_source}, 100)\n",
    "system = System(sampler, [pos_to_vel, vel_to_acc, acc_to_vel, vel_to_pos])\n",
    "\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "332ea08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8fb41e51984e96a3b1e6d795b99d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(axes=[Axis(label='timestamp', scale=LinearScale()), Axis(label='mouse_pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genki_signals.frontends import Line, WidgetFrontend\n",
    "\n",
    "pos =  Line(\"timestamp\", \"mouse_pos\")\n",
    "pos2 = Line(\"timestamp\", \"mouse_pos_2\")\n",
    "vel = Line(\"timestamp\", \"mouse_vel\")\n",
    "vel2 = Line(\"timestamp\", \"mouse_vel_2\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [pos, pos2, vel, vel2])\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "481ebdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cf006",
   "metadata": {},
   "source": [
    "Note that the \"position\" we end up with after differentiating twice and itegrating twice again is a very poor approximation, and drifts a lot. The reason for this is that any small errors in the twice-differentiated series (acceleration) due to numerical inaccuracies etc. get compounded when integrating. When integrating twice, the error increases as time squared.\n",
    "\n",
    "Note that this method will store all intermediate results. To prevent that we can wrap these functions with `Combine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cd7fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_acc_to_pos = Combine([pos_to_vel, vel_to_acc, acc_to_vel, vel_to_pos], name=\"mouse_pos_2\")\n",
    "\n",
    "system = System(sampler, [pos_to_acc_to_pos])\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3dcd4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2c9e22ee914ef9b70a9cfeeafe335d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(axes=[Axis(label='timestamp', scale=LinearScale()), Axis(label='mouse_pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genki_signals.frontends import Line, WidgetFrontend\n",
    "\n",
    "pos =  Line(\"timestamp\", \"mouse_pos\")\n",
    "pos2 = Line(\"timestamp\", \"mouse_pos_2\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [pos, pos2])\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83617bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f675c95",
   "metadata": {},
   "source": [
    "Lastly let's create a custom `SignalFunction`. \n",
    "\n",
    "To do that we need to extend the `SignalFunction` base class which takes three arguments input_signals, name and params. This is done for serialization purposes which we will cover further in the next example notebook\n",
    "\n",
    "Then we only need to implement a `__call__` method for our class.\n",
    "\n",
    "This method takes in a batch of samples of our input_signals and returns a batch of outputs. \n",
    "\n",
    "The signals passed into the function are defined in the `__init__` method (order matters).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d926c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from genki_signals.functions import SignalFunction\n",
    "from scipy import ndimage, signal\n",
    "\n",
    "class ConvGrayscaleImage(SignalFunction):\n",
    "    def __init__(self, input_signal, name, kernel, inverse=False):\n",
    "        super().__init__(input_signal, name=name, params={\"kernel\": kernel, \"inverse\": inverse})\n",
    "        self.kernel = kernel / np.linalg.norm(kernel)\n",
    "        self.inverse = inverse\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        grayscale = np.average(signal, axis=0, weights=[0.2989, 0.5870, 0.1140])\n",
    "        l = [ndimage.convolve(grayscale[..., i], self.kernel) for i in range(grayscale.shape[-1])]\n",
    "        if self.inverse:\n",
    "            return 255 - np.stack(l, axis=-1)\n",
    "        return np.stack(l, axis=-1)\n",
    "\n",
    "\n",
    "def gaussian_kernel(n, std):\n",
    "    '''\n",
    "    Generates a n x n matrix with a centered gaussian \n",
    "    of standard deviation std centered on it. If normalised,\n",
    "    its volume equals 1.'''\n",
    "    gaussian1D = signal.gaussian(n, std)\n",
    "    gaussian2D = np.outer(gaussian1D, gaussian1D)\n",
    "    return gaussian2D\n",
    "\n",
    "kernel = np.array([[-1, -1, -1, -1, -1],\n",
    "                   [-1,  1,  2,  1, -1],\n",
    "                   [-1,  2,  4,  2, -1],\n",
    "                   [-1,  1,  2,  1, -1],\n",
    "                   [-1, -1, -1, -1, -1]])\n",
    "\n",
    "gaussian = gaussian_kernel(5, 1)\n",
    "\n",
    "gaussian_edges = ndimage.convolve(gaussian, kernel)\n",
    "\n",
    "conv = ConvGrayscaleImage(\"video\", \"video_edges\", gaussian_edges, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb512a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = CameraSource(0)\n",
    "sampler = Sampler({\"video\": video}, sample_rate=60)\n",
    "system = System(sampler, [conv])\n",
    "\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52bef09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50577107ac14feb872900851370c732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg'), Image(value=b'', format='jpeg'))), HBox()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_normal = Video(\"video\")\n",
    "video_edges = Video(\"video_edges\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [video_normal, video_edges])\n",
    "\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88665ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f99d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d1ca8cbf69155084332556ae3352aa9e7bf4a96dd6bb5cc51f4289812d36157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
