{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337c43a4",
   "metadata": {},
   "source": [
    "# Signal functions in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7ec09",
   "metadata": {},
   "source": [
    "* Geometry example, raw acc/gyro data from Wave and compute pose, 3D cube? Gravity? Linear acceleration?\n",
    "* Filtering / spectrogram example with mic data / wave forms\n",
    "* ML inference example\n",
    "* Windowed signal functions: delay? FFT?\n",
    "* Creating custom signal functions, serialization quirks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e373fab",
   "metadata": {},
   "source": [
    "Genki Signals offers a wide range of `SignalFunction`s and this notebook is mostly meant to demonstrate some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: this isn't required when the library has been installed from PyPI\n",
    "os.chdir('../')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e49de56",
   "metadata": {},
   "source": [
    "`SignalFunction`s are functions on signal and can be classified into two classes:\n",
    "\n",
    "    1. one-to-one\n",
    "    2. many-to-one (Windowed)\n",
    "\n",
    "one-to-one `SignalFunction`s take in one sample and return one sample. They can still depend on former samples e.g. `Differentiate` but the number of samples out should be the same as number of samples in.\n",
    "\n",
    "many-to-one/Windowed `SignalFunction`s take in multiple samples and return one sample. They can still receive a single sample at a time but will only return a value when the window has been filled. As well as defining what to do with the window they need to define the window overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.system import System\n",
    "from genki_signals.sources import *\n",
    "from genki_signals.functions import *\n",
    "from genki_signals.frontends import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "252077a8",
   "metadata": {},
   "source": [
    "A good example of a windowed signal is short-time Fourier transform (stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d75864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = MicSource()\n",
    "stft = FourierTransform(\"audio\", \"fourier\", window_size=1024, window_overlap=512)\n",
    "fourier_system = System(mic, [stft])\n",
    "\n",
    "fourier_system.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd64554b",
   "metadata": {},
   "source": [
    "One way to view data from a `System` is to connect it to a buffer with `System.register_data_feed`. This can be useful to see if a `SignalFunction` is behaving properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.buffers import DataBuffer\n",
    "\n",
    "buffer = DataBuffer()\n",
    "\n",
    "fourier_system.register_data_feed(id(buffer), lambda d: buffer.extend(d))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02d96659",
   "metadata": {},
   "source": [
    "Now all data that goes through our `System` will be sent to the buffer. Below we can view our buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_system.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8b23ca",
   "metadata": {},
   "source": [
    "Another example of a `SignalFunction` are the `Inference` / `WindowedInference` functions which take in a onnx file and calculate inference on it.\n",
    "\n",
    "Let's load a model which predicts the relative depth of an image.\n",
    "\n",
    "Note: You can also create your own `Inference` function but more on custom `SignalFunctions` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#load the model from torch.hub\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "\n",
    "input_resolution = (256, 256)\n",
    "dummy_input = torch.randn((1, 3, *input_resolution))\n",
    "model_path = \"./examples/midas.onnx\"\n",
    "\n",
    "# export the model to onnx\n",
    "torch.onnx.export(\n",
    "    midas,\n",
    "    dummy_input,\n",
    "    model_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    # dynamic_axes=({\"input\": [0]})\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5e12355",
   "metadata": {},
   "source": [
    "To connect the model to our camera we need to create a `CameraSource`, a `Sampler` to sample it, and connect it to a `System` which also computes the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f49990",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraSource(resolution=input_resolution)\n",
    "camera_sampler = Sampler({\"model_input\": camera}, 10)\n",
    "model_inference = Inference(\"model_input\", \"model_output\", model_path, stateful=False)\n",
    "inference_system = System(camera_sampler, [model_inference], update_rate=50)\n",
    "inference_system.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96cba5e5",
   "metadata": {},
   "source": [
    "Now the depth image is under the key \"model_output\" and we can visualize it with our `WidgetFrontend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bdc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.frontends import Video, WidgetFrontend\n",
    "\n",
    "video = Video(\"model_input\")\n",
    "depth = Video(\"model_output\")\n",
    "\n",
    "frontend = WidgetFrontend(inference_system, [video, depth])\n",
    "\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_system.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b68aee6b",
   "metadata": {},
   "source": [
    "Notes on using multiple signal functions:\n",
    "\n",
    "If one `SignalFunction` depends on the output of another `SignalFunction` then it needs to come after the other in the initialization of the `System`\n",
    "\n",
    "An example of a sequence of `SignalFunction`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_vel = Differentiate(\"mouse_pos\", \"timestamp\", \"mouse_vel\")\n",
    "vel_to_acc = Differentiate(\"mouse_vel\", \"timestamp\", \"mouse_acc\")\n",
    "acc_to_vel = Integrate(\"mouse_acc\", \"timestamp\", \"mouse_vel_2\")\n",
    "vel_to_pos = Integrate(\"mouse_vel_2\", \"timestamp\", \"mouse_pos_2\", use_trapz=False)\n",
    "\n",
    "mouse_source = MouseSource()\n",
    "sampler = Sampler({\"mouse_pos\": mouse_source}, 100)\n",
    "system = System(sampler, [pos_to_vel, vel_to_acc, acc_to_vel, vel_to_pos])\n",
    "\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ea08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.frontends import Line, WidgetFrontend\n",
    "\n",
    "pos =  Line(\"timestamp\", \"mouse_pos\")\n",
    "pos2 = Line(\"timestamp\", \"mouse_pos_2\")\n",
    "vel = Line(\"timestamp\", \"mouse_vel\")\n",
    "vel2 = Line(\"timestamp\", \"mouse_vel_2\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [pos, pos2, vel, vel2])\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ebdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b02cf006",
   "metadata": {},
   "source": [
    "Note that this method will store all intermediate results.\n",
    "\n",
    "To prevent that we can wrap these functions in the `SignalFunction`: `Combine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_acc_to_pos = Combine([pos_to_vel, vel_to_acc, acc_to_vel, vel_to_pos], name=\"mouse_pos_2\")\n",
    "\n",
    "system = System(sampler, [pos_to_acc_to_pos])\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcd4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.frontends import Line, WidgetFrontend\n",
    "\n",
    "pos =  Line(\"timestamp\", \"mouse_pos\")\n",
    "pos2 = Line(\"timestamp\", \"mouse_pos_2\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [pos, pos2])\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83617bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f675c95",
   "metadata": {},
   "source": [
    "Lastly let's create a custom `SignalFunction`. \n",
    "\n",
    "To do that we need to extend the `SignalFunction` base class which takes three arguments input_signals, name and params. This is done for serialization purposes which we will cover further in the next example notebook\n",
    "\n",
    "Then we only need to implement a `__call__` method for our class.\n",
    "\n",
    "This method takes in a batch of samples of our input_signals and returns a batch of outputs. \n",
    "\n",
    "The signals passed into the function are defined in the `__init__` method (order matters).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from genki_signals.functions import SignalFunction\n",
    "from scipy import ndimage, signal\n",
    "\n",
    "class ConvGrayscaleImage(SignalFunction):\n",
    "    def __init__(self, input_signal, name, kernel, inverse=False):\n",
    "        super().__init__(input_signal, name=name, params={\"kernel\": kernel, \"inverse\": inverse})\n",
    "        self.kernel = kernel / np.linalg.norm(kernel)\n",
    "        self.inverse = inverse\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        grayscale = np.average(signal, axis=0, weights=[0.2989, 0.5870, 0.1140])\n",
    "        l = [ndimage.convolve(grayscale[..., i], self.kernel) for i in range(grayscale.shape[-1])]\n",
    "        if self.inverse:\n",
    "            return 255 - np.stack(l, axis=-1)\n",
    "        return np.stack(l, axis=-1)\n",
    "\n",
    "\n",
    "def gaussian_kernel(n, std):\n",
    "    '''\n",
    "    Generates a n x n matrix with a centered gaussian \n",
    "    of standard deviation std centered on it. If normalised,\n",
    "    its volume equals 1.'''\n",
    "    gaussian1D = signal.gaussian(n, std)\n",
    "    gaussian2D = np.outer(gaussian1D, gaussian1D)\n",
    "    return gaussian2D\n",
    "\n",
    "kernel = np.array([[-1, -1, -1, -1, -1],\n",
    "                   [-1,  1,  2,  1, -1],\n",
    "                   [-1,  2,  4,  2, -1],\n",
    "                   [-1,  1,  2,  1, -1],\n",
    "                   [-1, -1, -1, -1, -1]])\n",
    "\n",
    "gaussian = gaussian_kernel(5, 1)\n",
    "\n",
    "gaussian_edges = ndimage.convolve(gaussian, kernel)\n",
    "\n",
    "conv = ConvGrayscaleImage(\"video\", \"video_edges\", gaussian_edges, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb512a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = CameraSource(0)\n",
    "sampler = Sampler({\"video\": video}, sample_rate=60)\n",
    "system = System(sampler, [conv])\n",
    "\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bef09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_normal = Video(\"video\")\n",
    "video_edges = Video(\"video_edges\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [video_normal, video_edges])\n",
    "\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88665ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d1ca8cbf69155084332556ae3352aa9e7bf4a96dd6bb5cc51f4289812d36157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
