{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337c43a4",
   "metadata": {},
   "source": [
    "# Signal functions in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49de56",
   "metadata": {},
   "source": [
    "`SignalFunction`s are functions that take in one or more sigals and return a new signal. They can be one of to types:\n",
    "\n",
    "* one-to-one functions take in one sample and return one sample. They can be stateful and depend on previous samples, but the number of samples out should always be the same as number of samples in.\n",
    "* many-to-one (Windowed) functions operate on a sliding window of samples. The windows can overlap, but the function only returns one value per window. They can still receive a single sample at a time but will only return a value when the window has been filled. As well as defining what to do with the whow to process a window they need to define the window length and window overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb63c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.system import System\n",
    "from genki_signals.sources import MicSource, CameraSource, Sampler, MouseSource\n",
    "import genki_signals.functions as f\n",
    "from genki_signals.frontends import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252077a8",
   "metadata": {},
   "source": [
    "A good example of a windowed signal function is short-time Fourier transform (stft) which computes the Discrete Fourier transform on a sliding window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d75864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = MicSource()\n",
    "stft = f.FourierTransform(\"audio\", \"fourier\", window_size=1024, window_overlap=512)\n",
    "fourier_system = System(mic, [stft])\n",
    "\n",
    "fourier_system.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64554b",
   "metadata": {},
   "source": [
    "One way to view data from a `System` is to connect it to a buffer with `System.register_data_feed`. This can be useful to see if a `SignalFunction` is behaving properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f273f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genki_signals.buffers import DataBuffer\n",
    "\n",
    "buffer = DataBuffer()\n",
    "\n",
    "fourier_system.register_data_feed(id(buffer), lambda d: buffer.extend(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d96659",
   "metadata": {},
   "source": [
    "Now all data that goes through our `System` will be sent to the buffer. Below we can view our buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84b7df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBuffer(max_size=None, data=timestamp: (31,)\n",
       "audio: (31744,)\n",
       "fourier: (513, 62))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfeb6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b23ca",
   "metadata": {},
   "source": [
    "Note that in this case, the time series called `'fourier'` is shorter than `'audio'` - it is downsampled. The `DataBuffer` will accept data like this, but with great power comes great reponsibility. This can be a source of bugs since one tends to assume that timeseries grouped together are operating on the same frequency.\n",
    "\n",
    "Another example of a `SignalFunction` are the `Inference` and `WindowedInference` functions which take in a onnx file and calculate real-time inference with the neural network.\n",
    "\n",
    "Let's load a model which predicts the relative depth of an image. For this example to work you need to have [pytorch](https://pytorch.org/) and [timm](https://pypi.org/project/timm/) installed.\n",
    "\n",
    "(Note: You can also create your own `Inference` function but more on custom `SignalFunctions` later.)\n",
    "\n",
    "We start by loading the model and creating an ONNX file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a7eb1e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/egill/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/Users/egill/opt/miniconda3/envs/genki/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/egill/opt/miniconda3/envs/genki/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: (__ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE)\n",
      "  Referenced from: '/Users/egill/opt/miniconda3/envs/genki/lib/python3.9/site-packages/torchvision/image.so'\n",
      "  Expected in: '/Users/egill/opt/miniconda3/envs/genki/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/egill/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
      "/Users/egill/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master/geffnet/conv2d_layers.py:47: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return max((-(i // -s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
      "/Users/egill/opt/miniconda3/envs/genki/lib/python3.9/site-packages/torch/onnx/utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/Users/egill/opt/miniconda3/envs/genki/lib/python3.9/site-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#load the model from torch.hub\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "\n",
    "input_resolution = (256, 256)\n",
    "dummy_input = torch.randn((1, 3, *input_resolution))\n",
    "model_path = \"./midas.onnx\"\n",
    "\n",
    "# export the model to onnx\n",
    "torch.onnx.export(\n",
    "    midas,\n",
    "    dummy_input,\n",
    "    model_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    # dynamic_axes=({\"input\": [0]})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12355",
   "metadata": {},
   "source": [
    "To connect the model to our camera we need to create a `CameraSource`, a `Sampler` to sample it, and connect it to a `System` which also computes the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f49990",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraSource(resolution=input_resolution)\n",
    "camera_sampler = Sampler({\"model_input\": camera}, 10)\n",
    "model_inference = f.Inference(\"model_input\", \"model_output\", model_path, stateful=False)\n",
    "inference_system = System(camera_sampler, [model_inference], update_rate=50)\n",
    "inference_system.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cba5e5",
   "metadata": {},
   "source": [
    "The `Inference` function takes in an input name, output name, model path, and the boolean parameter `stateful`. When `stateful` is `False`, the model works on one input sample at a time independently. \n",
    "\n",
    "With `stateful=True`, the model is assumed to also take in a state vector as input, and output a new state vector along with the output (i.e. it is a Recurrent Neural Network), allowing it to operate with some historical context.\n",
    "\n",
    "Now the depth image is under the key \"model_output\" and we can visualize it with our `WidgetFrontend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4bdc3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9b797042014810971d3765b6c24674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg'), Image(value=b'', format='jpeg'))), HBox()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genki_signals.frontends import Video, WidgetFrontend\n",
    "\n",
    "video = Video(\"model_input\")\n",
    "depth = Video(\"model_output\")\n",
    "\n",
    "frontend = WidgetFrontend(inference_system, [video, depth])\n",
    "\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4171b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68aee6b",
   "metadata": {},
   "source": [
    "Notes on using multiple signal functions:\n",
    "\n",
    "If one `SignalFunction` depends on the output of another `SignalFunction` then it needs to come after the other in the list of functions when initializing the `System`\n",
    "\n",
    "An example of a sequence of `SignalFunction`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51a1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_vel = f.Differentiate(\"mouse_pos\", \"timestamp\", \"mouse_vel\")\n",
    "vel_to_acc = f.Differentiate(\"mouse_vel\", \"timestamp\", \"mouse_acc\")\n",
    "acc_to_vel = f.Integrate(\"mouse_acc\", \"timestamp\", \"mouse_vel_2\")\n",
    "vel_to_pos = f.Integrate(\"mouse_vel_2\", \"timestamp\", \"mouse_pos_2\", use_trapz=False)\n",
    "\n",
    "mouse_source = MouseSource()\n",
    "sampler = Sampler({\"mouse_pos\": mouse_source}, 100)\n",
    "system = System(sampler, [pos_to_vel, vel_to_acc, acc_to_vel, vel_to_pos])\n",
    "\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "332ea08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ddf367371f40fdaac791b57c826bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(axes=[Axis(label='timestamp', scale=LinearScale()), Axis(label='mouse_pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genki_signals.frontends import Line, WidgetFrontend\n",
    "\n",
    "pos =  Line(\"timestamp\", \"mouse_pos\")\n",
    "pos2 = Line(\"timestamp\", \"mouse_pos_2\")\n",
    "vel = Line(\"timestamp\", \"mouse_vel\")\n",
    "vel2 = Line(\"timestamp\", \"mouse_vel_2\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [pos, pos2, vel, vel2])\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481ebdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cf006",
   "metadata": {},
   "source": [
    "Note that the \"position\" we end up with after differentiating twice and itegrating twice again is a very poor approximation, and drifts a lot. The reason for this is that any small errors in the twice-differentiated series (acceleration) due to numerical inaccuracies etc. get compounded when integrating. When integrating twice, the error increases proportional to time squared.\n",
    "\n",
    "Note that this method will store all intermediate results. To prevent that we can wrap these functions with `Combine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd7fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_acc_to_pos = f.Combine([pos_to_vel, vel_to_acc, acc_to_vel, vel_to_pos], name=\"mouse_pos_2\")\n",
    "\n",
    "system = System(sampler, [pos_to_acc_to_pos])\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dcd4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28a7627bae84291a73ad4a67fd7e01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Figure(axes=[Axis(label='timestamp', scale=LinearScale()), Axis(label='mouse_pos…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genki_signals.frontends import Line, WidgetFrontend\n",
    "\n",
    "pos =  Line(\"timestamp\", \"mouse_pos\")\n",
    "pos2 = Line(\"timestamp\", \"mouse_pos_2\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [pos, pos2])\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83617bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f675c95",
   "metadata": {},
   "source": [
    "Lastly let's create a custom `SignalFunction`. Our function will be an image filter, implemented by convolving some kernel with an input (image) signal. It will also turn the images into grayscale.\n",
    "\n",
    "To do that we need to extend the `SignalFunction` base class. We technically only need to implement a `__call__` method for our class, but we will also override the default `__init__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c04d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from genki_signals.functions import SignalFunction\n",
    "from scipy import ndimage\n",
    "\n",
    "class ConvGrayscaleImage(SignalFunction):\n",
    "    def __init__(self, input_signal, name, kernel, inverse=False):\n",
    "        super().__init__(input_signal, name=name, params={\"kernel\": kernel, \"inverse\": inverse})\n",
    "        self.kernel = kernel / np.linalg.norm(kernel)\n",
    "        self.inverse = inverse\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        grayscale = np.average(signal, axis=0, weights=[0.2989, 0.5870, 0.1140])\n",
    "        l = [ndimage.convolve(grayscale[..., i], self.kernel) for i in range(grayscale.shape[-1])]\n",
    "        if self.inverse:\n",
    "            return 255 - np.stack(l, axis=-1)\n",
    "        return np.stack(l, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb66e9f",
   "metadata": {},
   "source": [
    "The `__call__` method will receive an image array of shape `(n_channels, height, width, time)`. This shape depends on the input signal we define, but the main rule is that the last dimension is reserved for time. We start by making the image grayscale by averaging over the `n_channels` dimension. We then compute the convolved images, one per sample (`time` dimension). The `inverse` parameter allows us to also invert the colors of the image. Finally, we recreate the `time` axis using `np.stack()`\n",
    "\n",
    "In the `__init__` method we call `super().__init__()` with some arguments, this needs to be done in a specific way. We will explain this in a bit, but first let's try our function in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d926c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def gaussian_kernel(n, std):\n",
    "    \"\"\"\n",
    "    Generates an n x n matrix with a centered gaussian \n",
    "    of standard deviation std centered on it.\n",
    "    \"\"\"\n",
    "    gaussian1D = signal.gaussian(n, std)\n",
    "    gaussian2D = np.outer(gaussian1D, gaussian1D)\n",
    "    return gaussian2D\n",
    "\n",
    "kernel = np.array([[-1, -1, -1, -1, -1],\n",
    "                   [-1,  1,  2,  1, -1],\n",
    "                   [-1,  2,  4,  2, -1],\n",
    "                   [-1,  1,  2,  1, -1],\n",
    "                   [-1, -1, -1, -1, -1]])\n",
    "\n",
    "gaussian = gaussian_kernel(5, 1)\n",
    "gaussian_edges = ndimage.convolve(gaussian, kernel)\n",
    "\n",
    "conv = ConvGrayscaleImage(\"video\", \"video_edges\", gaussian_edges, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb512a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = CameraSource(0)\n",
    "sampler = Sampler({\"video\": video}, sample_rate=60)\n",
    "system = System(sampler, [conv])\n",
    "\n",
    "system.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bef09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dd048976124b008ee4522e55fbedd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg'), Image(value=b'', format='jpeg'))), HBox()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_normal = Video(\"video\")\n",
    "video_edges = Video(\"video_edges\")\n",
    "\n",
    "frontend = WidgetFrontend(system, [video_normal, video_edges])\n",
    "\n",
    "frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88665ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d769fdc",
   "metadata": {},
   "source": [
    "All signal functions can be serialized into JSON, this is very useful for reasons we will explain in the next notebook, but it does have some nuances. This is the reason for the particular call to `__init__` we saw earlier:\n",
    "\n",
    "        super().__init__(input_signal, name=name, params={\"kernel\": kernel, \"inverse\": inverse})\n",
    "\n",
    "Each signal function must be initialized with these arguments in this order. The name of its input signal or signals, followed by a name given to the resulting signal, and finally a dict of extra parameters required to make the signal work. These three things, along with the class name, should uniquely specify a signal function and enable us to serialize and deserialize them. \n",
    "\n",
    "For example, the serialized JSON for our `conv` function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1debad6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__signal_function__\": true,\n",
      "    \"type\": \"ConvGrayscaleImage\",\n",
      "    \"inputs\": [\n",
      "        \"video\"\n",
      "    ],\n",
      "    \"name\": \"video_edges\",\n",
      "    \"params\": {\n",
      "        \"kernel\": [\n",
      "            [\n",
      "                -4.374211906893464,\n",
      "                -2.263119841223565,\n",
      "                -0.9742820850733217,\n",
      "                -2.2631198412235647,\n",
      "                -4.374211906893463\n",
      "            ],\n",
      "            [\n",
      "                -2.2631198412235642,\n",
      "                2.3801394855714437,\n",
      "                5.231474347618907,\n",
      "                2.3801394855714437,\n",
      "                -2.2631198412235647\n",
      "            ],\n",
      "            [\n",
      "                -0.9742820850733219,\n",
      "                5.2314743476189065,\n",
      "                9.052479364894262,\n",
      "                5.2314743476189065,\n",
      "                -0.9742820850733219\n",
      "            ],\n",
      "            [\n",
      "                -2.263119841223565,\n",
      "                2.3801394855714433,\n",
      "                5.231474347618907,\n",
      "                2.3801394855714433,\n",
      "                -2.2631198412235642\n",
      "            ],\n",
      "            [\n",
      "                -4.374211906893464,\n",
      "                -2.2631198412235647,\n",
      "                -0.9742820850733221,\n",
      "                -2.263119841223565,\n",
      "                -4.374211906893464\n",
      "            ]\n",
      "        ],\n",
      "        \"inverse\": true\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from genki_signals.functions.serialization import encode_signal_fn\n",
    "\n",
    "print(json.dumps(conv, default=encode_signal_fn, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6855cba",
   "metadata": {},
   "source": [
    "This is enough to recreate the exact same function (provided the underlying code doesn't change) and thus persist a collection of such functions to disk. \n",
    "\n",
    "The next notebook will dive a bit deeper into the `System` class and data recording, and how this serialization property of signal functions can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae03d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d1ca8cbf69155084332556ae3352aa9e7bf4a96dd6bb5cc51f4289812d36157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
